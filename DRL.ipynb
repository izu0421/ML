{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DRL.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"oxhUSogqmof_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"gLEoBss_mw5x","colab_type":"text"},"cell_type":"markdown","source":["#introduction \n","\n","- Deep reinforcement learning: adaptive, extremen trial-and-error --> combine with neural networks \n","- connection to neuroscience: make machine act smart --> postulate hypotheses with regarding to neuroscience \n","- reinforcement learning: deal with evolving dataset --> make sense of all of experiences that model collected --> world changing in time \n","- supervised = targeted, models that work \n","- unsupervised: need to find structure within data in itself (no clear supervision signal)\n","- examples: control power station with a view of reducing energy, trade the financial markets etc. \n","- RL can be seen as a form of supervised control: define the goal \n","- Hard games: need to \"bluff\" in poker, in eg starcraft --> delayed reward: many possible strategies- \n","\n","Human level control through reinforcement learning \n","- form of Q learning in monleys\n","- basal ganglia exhibit actor-critic architecture \n","- evolutionary methods also work in RL --> evolution to generate things \n","- agent learns faster when separated times (snippets of experiences) \n","\n","#Principles \n","- environemnt \n","- agent: agent's job is to maximize cumulative reward \n","- state: all the information input into the machine learning eg position of the pieces of the board, whose turn it is to play etc. \n","- reward: number that varies in time --> how well the agent is doing in time \n","\n","\n","Reward hypothesis: all goals can be described by the maximization of the expected cumulative reward eg investment portfolio: reward is the change in dollar market value of the portfolio \n","\n","Sequential decision making: \n","- goal: select actions to maximize total future reward \n","- actions may have longterm consequences \n","- reward may be delayed \n","- practice delayed gratification + sacrific immediate reward for longer-time gain \n","\n","Agent and environment: \n","- execute action A \n","- etc. \n","\n","History, transitions, state\n","- history is the sequence of transitions \n","- transition is a striplet state: state - action - reward \n","- state = function of history \n","\n","Markov state: theory of memoryless processes \n","- future is independent of the past given the present \n","- conditional probability of the transition \n","- history of the present contained in the present state \n","- state is a sufficient statistic of the future \n","- memory-less: can expend the state of formation --> by adding information to the state, agent can be Markov --> memory would be what's irrelevent for decision making \n","\n","Major components: abstractions that will help to build agent \n","- policy: determinstic vs stochastic --> depend on current state, refined as gain more information \n","- value function: prediction of future reward, used to exvaluate goodness/badness of states --> to select action \n","- model: prediction of the environment (next state + reward) \n","\n","Types of RL agents: \n","- value-based:\n","- policy based: \n","- actor-critic based:policy + value based \n","- model based vs model free: imagine next state \n","- use neural network to train a concept eg action of taking correction steps \n","- agent need to discover good policy from interacting with the environment \n","\n","Exploration = find more information about the environment <br> \n","Exploitation = maximize reward <br> \n","\n","Markov \n","- markov chains: source vs target states with P(transition) & there are probability to transfer bw states \n","- creating model = computing P(transitions) \n","- add reward: Markov reward process eg in 3 state markov chain S1 --> S2 --> S3, reward related to each state \n","- MDP Markov decision process: add actions to Markov reward process\n","- transition between S1-->S2, reward obtained in the process \n","- Partially Observable Markov Decision Processes POMDP: eg poker vs chess --> layer of uncertainty \n","\n","#Q-learning \n","\n","Q and V \n","- V = value of state --> total reward for being in a state \n","- expectation of sum of reward: if execute policy, will obtain reward \n","- Q = quality, numbers go one step beyond --> action-value functions: next action is a strong predictor of future reward (V) \n","- policy: how well the agent can act \n","\n","Optimal value function: optimal Q and V attainable by perfect controller \n","\n","Q-learning in practice: \n","- optimal action-values Q*(s,a) \n","- Q learning: learning the numbers that enable us to act \n","- Bellman equation: if we are in optimality, then can take the optimal option --> if know Q* for final state, max is a good proxy for reward \n","- prone to local optimal: do not know all the choices --> epsilon = chance that ignore Q and choose to exploit f\n","- 99% act according to Q & 1% exploration \n","\n","neural network: \n","- imput = state; output = state-value function \n","- convolutional neural networks: look at data set to build hierarchial features \n","\n","Bootstrapping: assume that we are at optimality and use Bellman equation to train nerual network \n","- Bellman: take the max reward from state model\n","\n","open AI gym: create environment \n","\n","ReLU: "]}]}