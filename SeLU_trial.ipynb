{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SeLU_trial.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"bqBBRHkoEcVB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np      # effective math\n","import matplotlib.pyplot as plt     # ultimate plotting tool\n","from mpl_toolkits.mplot3d import Axes3D     # 3D plots\n","import pandas as pd     # allow us to make dataframes to store our data cleanly"],"execution_count":0,"outputs":[]},{"metadata":{"id":"emk1cJ6UEixD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from google.colab import files \n","files.upload()\n","\n","data = pd.read_csv('Iris.csv').set_index('Id')  # read our data into a dataframe and set the index to the ID\n","#the dataframe already have IDs \n","print(data)     # last column is a label, all other columns are features\n","\n","X = np.array(data[data.columns[:-1]])   # set our design matrix to the features (all columns except last)\n","label_dict = {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2}  # dictionary containing label to number mapping \n","#make a dictionary to allow simpler access to species' levels \n","Y = np.array([label_dict[i] for i in data[data.columns[-1]]])   # put out labels into a np array\n","#for each element in last column, refer the species to the appropriate label --> made into array \n","print(Y)\n","\n","print(X.shape)      # 150 rows (datapoints), 4 columns (features)\n","print(Y.shape)      # 150 single dimension labels\n","\n","m = X.shape[0]      # 150 rows"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l0IvRtQMEpEJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def normalise(x):\n","    \"\"\"Centre around mean and divide by range to put all features on similar scale\"\"\"\n","    x_std = x - np.mean(x, axis=0)      # subtract the mean\n","    x_std = np.divide(x_std, np.std(x_std, axis=0))     # divide each feature by the range of that feature (-1 < x < 1)\n","    return x_std    # return our standardised features\n","\n","X_std = normalise(X)    # centre data around mean and divide by range/s.d"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vLwHcNkAErRE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def decompose(x):\n","    \"\"\"Compute the covariance matrix of the data and find its eigen properties\"\"\"\n","    cov = np.matmul(x.T, x)     # compute the covariance matrix\n","    print('\\nCovariance matrix')\n","    print(cov)\n","\n","    eig_vals, eig_vecs = np.linalg.eig(cov)     # find the eigenvalues and eigenvectors of the covariance matrix\n","    print('\\nEigenvectors')\n","    print(eig_vecs)\n","    print('\\nEigenvalues')\n","    print(eig_vals)\n","    return eig_vals, eig_vecs, cov\n","\n","eig_vals, eig_vecs, covariance = decompose(X_std)      # compute the covariance matrix and find its characteristics"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mJwT0wcpE66I","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def whicheigs(eig_vals):\n","    \"\"\"\"Plot the variance accounted for by each eigenvector and their cumulative sum\"\"\"\n","    total = sum(eig_vals)   # sum up the eigenvalues so we can compare each one to the total to determine their importance\n","    var_percent = [(i/total) * 100 for i in eig_vals]   # calculate the percentage variance of the data which this eigenvalue accounts for\n","    cum_var_percent = np.cumsum(var_percent)    # make a vector of the cumulative sum of the variance percentages\n","\n","    fig = plt.figure()      # make a figure\n","    ax =  fig.add_subplot(111)      # add an axis\n","    plt.title('Variance along different principal components')\n","    ax.grid()\n","    plt.xlabel('Principal Component')\n","    plt.ylabel('Percentage total variance accounted for')\n","\n","    ax.plot(cum_var_percent, '-ro')     # plot the cumulative sum of the variances accounted for by each eigenvector\n","    ax.bar(range(len(eig_vals)), var_percent) # position, height # show how much variance individual eig accounts for\n","    plt.xticks(np.arange(len(eig_vals)), ('PC{}'.format(i) for i in range(len(eig_vals))))  # set the xticks to 'PC1' etc\n","    plt.show()  # show us the figure\n","    \n","whicheigs(eig_vals)     # visualise the variance of the data for each eigenvector of the covariance matrix"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M6xZ3WXaFDfz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["colour_dict = {0:'r', 1:'g', 2:'b'}     # map labels to colours for plotting\n","colour_list = [colour_dict[i] for i in list(Y)]     # generator to give list of colours corresponding to each class\n","\n","dim = 2 # variable to define how many dimensions we want to map our data to\n","\n","def plotreduced(x):\n","    \"\"\"Plot the data which has been transformed to a visualisable dimension\"\"\"\n","    fig = plt.figure()      # make a figure\n","    plt.grid()\n","    if dim == 3:\n","        ax = fig.add_subplot(111, projection='3d')      # add a 3d set of axes\n","        ax.scatter(x[:, 0], x[:, 1], x[:, 2], c=colour_list)    # scatter plot our 3d data\n","        plt.xlabel('PC1 value')\n","        plt.ylabel('PC2 value')\n","        ax.set_zlabel('PC3 value')\n","    elif dim == 2:\n","        ax = fig.add_subplot(111)      # add a 2d set of axes\n","        ax.scatter(x[:, 0], x[:, 1], c=colour_list)    # scatter plot our 3d data\n","        plt.xlabel('PC1 value')\n","        plt.ylabel('PC2 value')\n","    elif dim == 1:\n","        ax = fig.add_subplot(111)       # add a 2d axis\n","        ax.scatter(x, np.zeros_like(x), c=colour_list)      # plot the 1D data along the x axis (zero for each y value)\n","        plt.xlabel('PC1 Value')\n","    plt.show()\n","    return ax\n","\n","ax = plotreduced(X_reduced)      # check out how the data looks in a visualisable dimension"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EzX7KImbFnB-","colab_type":"text"},"cell_type":"markdown","source":["#weight initialization\n","If all of our weights have very high positive or negative values initially, then our neurons will satuarate and we will get very small gradients leading to slow learning or no learning at all. "]},{"metadata":{"id":"7xbTCHU1Flie","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class Linear(nn.Linear): #overwrite reset parameter of nn.Linear \n","    def reset_parameters(self):\n","        var = 2 / (self.in_features + self.out_features)\n","        self.weight.data.normal_(0, np.sqrt(var)) #takes in stdv as argument so we square-root variance to get stdv\n","        if self.bias is not None:\n","            self.bias.data.zero_()\n","\n","class Conv2d(nn.Conv2d):\n","    def reset_parameters(self):\n","        var = 2 / ((self.in_channels + self.out_channels) * np.prod(self.kernel_size))\n","        self.weight.data.normal_(0, np.sqrt(var))\n","        if self.bias is not None:\n","            self.bias.data.zero_()"],"execution_count":0,"outputs":[]}]}