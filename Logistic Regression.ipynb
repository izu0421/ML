{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day 1 - Logistic Regression.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"GzNX4cHT6loG","colab_type":"text"},"cell_type":"markdown","source":["# Logistic Regression\n","- classification model: assumes that the probability of each class is proportional to a weighted sum of the input features.\n","- tatistical method: analyse dataset with independent explanatory variables \n","- outcome measured with dichotomous variable: 2 possible outcomes\n","- similar to linear regression except we apply a sigmoid activation to our output to map it between 0 and 1 as it represents a probability\n","\n","\n","### $h = \\sigma(XW)$\n","\n","Mean Squared Error Loss would work for this problem too, but Cross Entropy Loss leads to faster convergence.\n","\n","### $ J = \\sum_{i=1}^{m} - y^{(i)} \\cdot \\text{log}(h^{(i)}) + (1-y^{(i)}) \\cdot \\text{log}(1-h^{(i)})$\n","\n","### Multi-class case\n","Binary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule.\n","\n","Consider the case where we are not performing binary classification but have multiple classes. How do we extend our regression model to work with this? We simply have multiple outputs where each output is a sigmoid applied to a linear combination of the inputs representing the probability of belonging to each class. This corresponds to adding an extra column in our weights vector so it is now a matrix. To calculate the cost we use the same cost function but sum the cost across all outputs.\n","\n","To represent the labels, we can't use a single scalar value representing the class number anymore since our output is a vector so we wouldn't be able to compare them. Instead, we use one-hot encode the label. This is when we have a vector of length K, where K is the number of classes, which has a value of 0 for all the numbers except for the label class number which has a value of 1. If you think about it intuitively, this makes sense as we want our model to predict a probability of 1 for belonging to the label class and 0 for all other classes.\n","\n","### Optimization\n","As usual, we use gradient descent to optimize this. Calculating our outputs and using that to calculate the gradient of the cost w.r.t each parameter so we can update the parameters to reduce the cost."]},{"metadata":{"id":"Eu2aS2db6loH","colab_type":"text"},"cell_type":"markdown","source":["## Implementation\n","We are going to be implementing a model that takes in 4 features which are measurements of a flower and it will predict which one of the three species it belongs to.\n","\n","We begin by importing the required libraries"]},{"metadata":{"id":"Pw4sulVv6loI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1},{"item_id":2}],"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":73},"outputId":"43256517-4768-45bd-d5ee-d4d56701dcd0","executionInfo":{"status":"ok","timestamp":1523013117175,"user_tz":-60,"elapsed":15656,"user":{"displayName":"Yizhou Yu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102264779306922821749"}}},"cell_type":"code","source":["from google.colab import files\n","\n","files.upload()\n","\n","import numpy as np\n","import torch\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-01712ae0-c0cd-46ea-8bc9-51e731c83866\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-01712ae0-c0cd-46ea-8bc9-51e731c83866\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving Iris.csv to Iris (2).csv\n"],"name":"stdout"}]},{"metadata":{"id":"4fOzgA0Oppkn","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# http://pytorch.org/\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n","import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NtxMnACV6loL","colab_type":"text"},"cell_type":"markdown","source":["Import the required dataset intro a pandas data frame. Map our class labels into numerical values and shuffle the dataset."]},{"metadata":{"id":"lgwyfhD56loM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":121},"outputId":"f90fbc7a-9430-498a-8d66-d6dfb10fd5c1","executionInfo":{"status":"ok","timestamp":1523013124235,"user_tz":-60,"elapsed":500,"user":{"displayName":"Yizhou Yu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102264779306922821749"}}},"cell_type":"code","source":["df = pd.read_csv('Iris.csv')\n","df[['Species']] = df['Species'].map({'Iris-setosa':0, 'Iris-virginica':1, 'Iris-versicolor':2}) #map text labels to numberical vaules\n","df = df.sample(frac=1) #shuffle our dataset\n","\n","print(df.head())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species\n","101  102            5.8           2.7            5.1           1.9        1\n","145  146            6.7           3.0            5.2           2.3        1\n","131  132            7.9           3.8            6.4           2.0        1\n","141  142            6.9           3.1            5.1           2.3        1\n","77    78            6.7           3.0            5.0           1.7        2\n"],"name":"stdout"}]},{"metadata":{"id":"LKEZ8S1P6loO","colab_type":"text"},"cell_type":"markdown","source":["Select the appropriate features and labels and convert them into torch tensors so we can use them in our model"]},{"metadata":{"id":"Nl2-fand6loP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["X = torch.Tensor(np.array(df[df.columns[1:-1]])) #pick our features from our dataset --> mot used ID as input feaures \n","\n","Y = torch.LongTensor(np.array(df[['Species']]).squeeze()) #select our label - squeeze() removes redundant dimensions"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VCxxbJpy6loQ","colab_type":"text"},"cell_type":"markdown","source":["Split the data into training and test sets and convert to variables so we can use them with the torch autograd library"]},{"metadata":{"id":"OueKbrka6loS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["m = 100 #size of training set\n","\n","#training set\n","x_train = Variable(X[0:m])\n","y_train = Variable(Y[0:m])\n","\n","#test set\n","x_test = Variable(X[m:])\n","y_test = Variable(Y[m:])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gwLFQ8db6loT","colab_type":"text"},"cell_type":"markdown","source":["Define the Logistic Model using PyTorch's class interface."]},{"metadata":{"id":"txraJ3mc6loU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class logisticmodel(torch.nn.Module): #inheriting from torch etc, similar model to linear regression + apply to progessions\n","    def __init__(self):\n","        super().__init__() #call parent class initializer\n","        self.linear = torch.nn.Linear(4, 3) #define linear combination function with 4 inputs and 3 outputs\n","\n","    def forward(self, x):\n","        x = self.linear(x) #linearly combine our inputs to give 3 outputs\n","        x = torch.nn.functional.softmax(x, dim=1) #activate our output neurons to give probabilities of belonging to each of the three class\n","        return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HKke9nEa6loW","colab_type":"text"},"cell_type":"markdown","source":["Define the training hyperparameters, cost function and optimizer. Instantiate a model from the class we defined earlier."]},{"metadata":{"id":"w887EhJf6loX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["no_epochs = 100\n","lr = 0.1\n","\n","mymodel = logisticmodel() #create our model from defined class\n","criterion = torch.nn.CrossEntropyLoss() #cross entropy cost function as it is a classification problem\n","optimizer = torch.optim.Adam(mymodel.parameters(), lr = lr) #define our optimizer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xb5KdAkY6loZ","colab_type":"text"},"cell_type":"markdown","source":["Define the axes which we will use to plot the costs. Define the function used to train the model and train it for the number of epochs specified earlier."]},{"metadata":{"id":"8rNB3WAD6loZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["costs=[] #store the cost each epoch\n","plt.ion()\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","ax.set_xlabel('Epoch')\n","ax.set_ylabel('Cost')\n","ax.set_xlim(0, no_epochs)\n","plt.show()\n","\n","#define train function used to train models \n","def train(no_epochs):\n","    for epoch in range(no_epochs):\n","        h = mymodel.forward(x_train) #forward propagate --> calulate our hypothesis\n","\n","        #calculate, plot and print cost\n","        cost = costf(h, y_train)\n","        costs.append(cost.data[0])\n","        ax.plot(costs, 'b')\n","        fig.canvas.draw()\n","        print('Epoch ', epoch, ' Cost: ', cost.data[0])\n","\n","        #calculate gradients + update weights using gradient descent step with our optimizer\n","        optimizer.zero_grad()\n","        cost.backward()\n","        optimizer.step()\n","\n","train(no_epochs) #train the model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CxAYewb_6loc","colab_type":"text"},"cell_type":"markdown","source":["Test the accuracy of the trained model --> new function that corresponds accurately"]},{"metadata":{"id":"p2CAVxUG6loe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["test_h = mymodel.forward(x_test) #predict probabilities for test set\n","_, test_h = test_h.data.max(1) #returns the output which had the highest probability\n","#the _ is a throw away variable \n","#not saving the parameters \n","test_y = y_test.data # get tensor stored in variable \n","correct = torch.eq(test_h, test_y) #perform element-wise equality operation\n","#1 for all model that got correct and 0 for when model is incorrect \n","\n","accuracy = torch.sum(correct)/correct.shape[0] #calculate accuracy -> percentage of correct \n","#correct.shape[0] = total length\n","\n","print('Test accuracy: ', accuracy)\n","#reducing the dimensions will reduce the accuracy? \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IokoF5dY6lok","colab_type":"text"},"cell_type":"markdown","source":["Save the optimized model parameters"]},{"metadata":{"id":"Kts8G_kr6lol","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["torch.save(mynet.state_dict(), 'trained_logistic_model')"],"execution_count":0,"outputs":[]}]}